import asyncio
import difflib
import json
import pandas as pd
import re
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration
# initialize the model and tokenizer
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("grammarly/coedit-large")
model = T5ForConditionalGeneration.from_pretrained("grammarly/coedit-large").to(device)

def fix_grammar(text: str, tokenizer, model, device) -> str:
    prompt = "Fix grammar: " + text
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    outputs = model.generate(inputs.input_ids, max_length=64)
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return output_text

def split_text_into_chunks(text: str, tokenizer, max_tokens: int = 64) -> list:
    """
    T√°ch text th√†nh c√°c chunk nh·ªè d·ª±a theo c√¢u, sao cho m·ªói chunk kh√¥ng v∆∞·ª£t qu√° max_tokens.
    S·ª≠ d·ª•ng regex ƒë·ªÉ t√°ch c√¢u d·ª±a tr√™n d·∫•u k·∫øt th√∫c c√¢u (., !, ?).
    """
    # T√°ch theo c√°c d·∫•u k·∫øt th√∫c c√¢u (bao g·ªìm c·∫£ kho·∫£ng tr·∫Øng ph√≠a sau)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        test_chunk = current_chunk + (" " if current_chunk else "") + sentence
        tokens = tokenizer.tokenize(test_chunk)
        if len(tokens) <= max_tokens:
            current_chunk = test_chunk
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def annotate_differences(original_text: str, corrected_text: str) -> str:
    original_words = original_text.split()
    corrected_words = corrected_text.split()

    seq_matcher = difflib.SequenceMatcher(None, original_words, corrected_words)
    opcodes = seq_matcher.get_opcodes()

    def get_word_positions(text: str):
        positions = []
        for match in re.finditer(r'\S+', text):
            start, end = match.span()
            positions.append((match.group(), start, end))
        return positions

    original_positions = get_word_positions(original_text)
    annotated_text = ""
    last_index = 0

    for tag, i1, i2, j1, j2 in opcodes:
        if tag == "equal":
            if i1 < len(original_positions):
                segment_start = original_positions[i1][1]
                segment_end = original_positions[i2 - 1][2] if (i2 - 1) < len(original_positions) else len(original_text)
                annotated_text += original_text[last_index:segment_start]
                annotated_text += original_text[segment_start:segment_end]
                last_index = segment_end
        elif tag in ["replace", "delete"]:
            if i1 < len(original_positions):
                segment_start = original_positions[i1][1]
                segment_end = original_positions[i2 - 1][2] if (i2 - 1) < len(original_positions) else len(original_text)
                error_segment = original_text[segment_start:segment_end]
                suggestion = " ".join(corrected_words[j1:j2])
                annotated_text += original_text[last_index:segment_start]
                annotated_text += (
                    f"<span class='error-block' onclick='showSuggestion(this)' data-suggestion='{suggestion}'>{error_segment}</span>"
                )
                last_index = segment_end
        elif tag == "insert":
            suggestion = " ".join(corrected_words[j1:j2])
            annotated_text += f"<span class='suggestion'>{suggestion}</span>"

    annotated_text += original_text[last_index:]
    return annotated_text


def process_document(document_text: str, tokenizer, model, device, max_tokens: int = 64) -> str:    
    """
    X·ª≠ l√Ω m·ªôt t√†i li·ªáu:
      - T√°ch t√†i li·ªáu th√†nh c√°c ƒëo·∫°n d·ª±a tr√™n c√°c k√Ω t·ª± xu·ªëng d√≤ng li√™n ti·∫øp,
        b·∫±ng c√°ch s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy c√≥ nh√≥m b·∫Øt ƒë·ªÉ gi·ªØ l·∫°i c√°c k√Ω t·ª± xu·ªëng d√≤ng g·ªëc.
      - V·ªõi m·ªói ƒëo·∫°n (nh·ªØng ƒëo·∫°n kh√¥ng ph·∫£i l√† ch·ªâ ch·ª©a k√Ω t·ª± xu·ªëng d√≤ng),
        n·∫øu s·ªë token v∆∞·ª£t qu√° max_tokens th√¨ t√°ch th√†nh c√°c chunk nh·ªè v√† x·ª≠ l√Ω t·ª´ng ph·∫ßn,
        c√≤n l·∫°i th√¨ x·ª≠ l√Ω tr·ª±c ti·∫øp.
      - Gh√©p l·∫°i k·∫øt qu·∫£ ƒë√£ annotate m√† v·∫´n gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng ban ƒë·∫ßu c·ªßa ng∆∞·ªùi d√πng (bao g·ªìm tab, kho·∫£ng tr·∫Øng, newlines).
    """
    # D√πng capturing group ƒë·ªÉ t√°ch c√°c ƒëo·∫°n v√† gi·ªØ l·∫°i delimiter (c√°c d√≤ng xu·ªëng li√™n ti·∫øp, c√≥ th·ªÉ c√≥ kho·∫£ng tr·∫Øng v√† tab)
    segments = re.split(r'(\n\s*\n)', document_text)
    annotated_segments = []
    
    for segment in segments:
        # N·∫øu segment ch·ªâ ch·ª©a c√°c k√Ω t·ª± xu·ªëng d√≤ng (v√† kho·∫£ng tr·∫Øng) th√¨ gi·ªØ nguy√™n
        if re.fullmatch(r'\n\s*\n', segment):
            annotated_segments.append(segment)
        else:
            # Gi·ªØ nguy√™n c·∫•u tr√∫c ban ƒë·∫ßu (kh√¥ng .strip() ƒë·ªÉ b·∫£o to√†n tab, kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu/ƒëu√¥i)
            text_segment = segment
            tokens = tokenizer.tokenize(text_segment)
            if len(tokens) > max_tokens:
                chunks = split_text_into_chunks(text_segment, tokenizer, max_tokens)
            else:
                chunks = [text_segment]
            
            annotated_chunks = []
            for chunk in chunks:
                print(chunk)
                corrected_chunk = fix_grammar(chunk, tokenizer, model, device)
                annotated_chunk = annotate_differences(chunk, corrected_chunk)
                annotated_chunks.append(annotated_chunk)
            # Gh√©p l·∫°i c√°c chunk x·ª≠ l√Ω c·ªßa ƒëo·∫°n ƒë√≥ (gi·ªØa c√°c chunk m√¨nh n·ªëi b·∫±ng m·ªôt kho·∫£ng tr·∫Øng ƒë∆°n)
            annotated_text = "".join(annotated_chunks)
            annotated_segments.append(annotated_text)
    
    # Gh√©p l·∫°i to√†n b·ªô c√°c segment theo ƒë√∫ng th·ª© t·ª± ban ƒë·∫ßu
    return "".join(annotated_segments)

def wrap_words_with_click(text: str) -> str:
    words = text.split()
    return ' '.join(
        f"<span class='word' onclick='scrollToWord({i})'>{w}</span>"
        for i, w in enumerate(words)
    )

def annotated_html_with_ids(original_text: str, annotated_html: str) -> str:
    original_words = original_text.split()
    for i, word in enumerate(original_words):
        annotated_html = re.sub(
            rf"(<span class='(error|suggestion)'[^>]*?>){re.escape(word)}(</span>)",
            rf"\1<span id='suggestion-word-{i}'>\2</span>\3",
            annotated_html,
            count=1
        )
    return annotated_html

def postprocess_annotated_html(annotated_html: str) -> str:
    # Normalize line endings
    normalized = annotated_html.replace('\r\n', '\n')
    # Replace single newlines inside paragraphs with space
    normalized = re.sub(r'(?<!\n)\n(?!\n)', ' ', normalized)
    # Split into paragraphs on double newlines
    paragraphs = normalized.strip().split('\n\n')
    # Wrap with <p> and add spacing
    return ''.join(f"<p style='margin-bottom: 0.75em'>{p.strip()}</p>" for p in paragraphs if p.strip())

async def get_annotated_fixed_essay(answer: str) -> str:
    original_text = answer.strip()
    annotated_html = process_document(original_text, tokenizer, model, device, max_tokens=64)
    annotated_html = annotated_html_with_ids(original_text, annotated_html)
    # üëâ Final post-processing before returning
    annotated_html = postprocess_annotated_html(annotated_html)

    return annotated_html

# TESTING WITH HTML (EDITABLE + CLICKABLE + TOTAL WORD COUNT (REAL TIME) + TOP 5 FREQUENT WORDS)
# async def main():
#     df = pd.read_csv('55_samples.csv')
#     question = df['question'][35]
#     answer = str(df['answer'][35]).strip()

#     print("type(answer):", type(answer))

#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     tokenizer = AutoTokenizer.from_pretrained("grammarly/coedit-large")
#     model = T5ForConditionalGeneration.from_pretrained("grammarly/coedit-large").to(device)

#     original_text = """In the recent years, many animals are facing extinction due to human activities. From Sumatran tiger to giant panda, they is losing their homes and lives every day. The main reasons includes habitat lost, climate change, illegal hunting and human expansion. It is important we protect endangered animals before it‚Äôs too late.

#     One big problem are deforestation. Forests are cut down for farming, building and other human purpose. This make animals have no place to live. For example, orangutans in Borneo and Sumatra has lost most of their forest. With no enough space, animals can not find food or meet other of their kind, which make it hard to survive."""

#     annotated_html = process_document(answer, tokenizer, model, device, max_tokens=64)

#     with open("grammar_feedback_editable.html", "w", encoding="utf-8") as f:
#         f.write(f"""
#         <!DOCTYPE html>
#         <html>
#         <head>
#             <meta charset="UTF-8">
#             <title>Editable Grammar Feedback</title>
#             <style>
#                 body {{
#                     font-family: system-ui, -apple-system, 'Segoe UI', Roboto, Arial, sans-serif;
#                     font-size: 16px;
#                     line-height: 1.6;
#                     padding: 30px;
#                 }}
#                 .editable {{
#                     border: 1px solid #ccc;
#                     padding: 15px;
#                     border-radius: 8px;
#                     min-height: 150px;
#                 }}
#                 .error-block {{
#                     text-decoration: underline;
#                     text-decoration-color: red;
#                     text-decoration-style: solid;
#                     text-decoration-thickness: 2px;
#                     text-underline-offset: 2px;
#                     cursor: pointer;
#                     position: relative;
#                 }}
#                 .popup {{
#                     position: absolute;
#                     background: #fefefe;
#                     border: 1px solid #ccc;
#                     padding: 4px 10px;
#                     border-radius: 6px;
#                     box-shadow: 0 2px 6px rgba(0,0,0,0.2);
#                     z-index: 100;
#                     white-space: nowrap;
#                     top: 100%;
#                     left: 0;
#                 }}
#             </style>
#             <script>
#                 function updateErrorCount() {{
#                     const count = document.querySelectorAll('.error-block').length;
#                     document.getElementById('error-count').innerText = count;
#                 }}

#                 function updateWordStats() {{
#                     const text = document.getElementById('editor').innerText;
#                     const words = text.match(/\\b\\w+\\b/g) || [];
#                     document.getElementById('word-count').innerText = words.length;

#                     const freq = {{}};
#                     words.forEach(w => {{
#                         w = w.toLowerCase();
#                         freq[w] = (freq[w] || 0) + 1;
#                     }});

#                     const top = Object.entries(freq)
#                         .sort((a, b) => b[1] - a[1])
#                         .slice(0, 5);

#                     const ul = document.getElementById('top-words');
#                     ul.innerHTML = top.map(([w, c]) => `<li>${{w}}: ${{c}}</li>`).join('');
#                 }}

#                 function showSuggestion(el) {{
#                     document.querySelectorAll('.popup').forEach(p => p.remove());
#                     const suggestion = el.getAttribute('data-suggestion');
#                     const popup = document.createElement('div');
#                     popup.className = 'popup';
#                     popup.innerText = suggestion;

#                     popup.onclick = function () {{
#                         el.outerText = suggestion;
#                         updateErrorCount();
#                         updateWordStats();
#                     }};

#                     el.appendChild(popup);
#                 }}

#                 document.addEventListener('DOMContentLoaded', () => {{
#                     updateErrorCount();
#                     updateWordStats();
#                     document.getElementById('editor').addEventListener('input', updateWordStats);
#                 }});
#             </script>
#         </head>
#         <body>
#             <h2>Editable Grammar Checker</h2>

#             <p><strong>Review suggestions:</strong> <span id="error-count">0</span></p>
#             <p><strong>Word count:</strong> <span id="word-count">0</span></p>
#             <p><strong>Top 5 frequent words:</strong></p>
#             <ul id="top-words"></ul>

#             <div id="editor" class="editable" contenteditable="true">
#                 {annotated_html}
#             </div>
#         </body>
#         </html>
#         """)

#     print("‚úÖ Saved to grammar_feedback_editable.html ‚Äì open it in your browser to interact.")


# if __name__ == "__main__":
#     asyncio.run(main())